{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-gram 최종.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callee2006/MachineLearning/blob/master/skip-gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwDBfJERa07",
        "colab_type": "text"
      },
      "source": [
        "# **Skip-gram with negative smapling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du0GK2oNRItO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b17jQohbvXhM",
        "colab_type": "text"
      },
      "source": [
        "## NLTK (Natural Language Toolkit)\n",
        "\n",
        "https://www.nltk.org/\n",
        "\n",
        "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZrLs03Yx3OJ",
        "colab_type": "code",
        "outputId": "61c1b22b-3f39-4137-97dd-d3d797884b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L92lZOsbRR01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FloatTensor = torch.FloatTensor\n",
        "LongTensor = torch.LongTensor\n",
        "ByteTensor = torch.ByteTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irN4kKwcRkqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "    random.shuffle(train_data)\n",
        "    sindex = 0\n",
        "    eindex = batch_size\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex: eindex]\n",
        "        temp = eindex\n",
        "        eindex = eindex + batch_size\n",
        "        sindex = temp\n",
        "        yield batch\n",
        "    \n",
        "    if eindex >= len(train_data):\n",
        "        batch = train_data[sindex:]\n",
        "        yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KO8YBrTRnMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Return the Tensor with index information corresponding to seq\n",
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))   # (함수정의, parameter)\n",
        "    return Variable(LongTensor(idxs))\n",
        "\n",
        "#Return the Tensor with index information corresponding to the word\n",
        "def prepare_word(word, word2index):\n",
        "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67k1BuKRpoX",
        "colab_type": "text"
      },
      "source": [
        "# **Data load and Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNFiZcQSRunn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500]\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fChJK81CTMnO",
        "colab_type": "text"
      },
      "source": [
        "**Exclude sparse words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3d718UOTQG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count = Counter(flatten(corpus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeFxpKBDxd3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_COUNT = 3\n",
        "exclude = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C-58KIRxf0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for w, c in word_count.items():\n",
        "    if c < MIN_COUNT:\n",
        "        exclude.append(w)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdqakO7-yCwT",
        "colab_type": "text"
      },
      "source": [
        "Prepare train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELs6M7VyEiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = list(set(flatten(corpus)) - set(exclude))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrWqRzyjyLbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {}\n",
        "for vo in vocab:\n",
        "    if word2index.get(vo) is None:\n",
        "        word2index[vo] = len(word2index)\n",
        "        \n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRoEerE6yOBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 5\n",
        "windows =  flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in corpus])\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for window in windows:\n",
        "    for i in range(WINDOW_SIZE * 2 + 1):\n",
        "        if window[i] in exclude or window[WINDOW_SIZE] in exclude: \n",
        "            continue # min_count\n",
        "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
        "            continue\n",
        "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
        "\n",
        "X_p = []\n",
        "y_p = []\n",
        "\n",
        "for tr in train_data:\n",
        "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
        "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))\n",
        "\n",
        "#change to tensor with index\n",
        "train_data = list(zip(X_p, y_p))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYLCe2AT6gOV",
        "colab_type": "text"
      },
      "source": [
        "# **Build Unigram Distribution ** 0.75**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj2_OxyH7UZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z = 0.001\n",
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGHIylEF7hmp",
        "colab_type": "code",
        "outputId": "36399f72-38dc-4520-be73-8379fdb5b9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "unigram_table = []\n",
        "\n",
        "for vo in vocab:\n",
        "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))\n",
        "\n",
        "print(unigram_table)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'that', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'or', 'world', 'world', 'world', 'world', 'world', 'world', 'dives', 'dives', 'picture', 'picture', 'picture', 'picture', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'came', 'came', 'came', 'came', 'came', 'them', 'them', 'them', 'them', 'them', 'them', 'them', 'them', 'them', 'among', 'among', 'among', 'among', 'marvellous', 'marvellous', 'captain', 'captain', 'captain', 'london', 'london', 'yet', 'yet', 'yet', 'yet', 'yet', 'yet', 'english', 'english', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'air', 'air', 'air', 'air', 'air', 'whom', 'whom', 'whom', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'perhaps', 'perhaps', 'better', 'better', 'better', 'kind', 'kind', 'summer', 'summer', 'having', 'having', 'having', 'against', 'against', 'against', 'brought', 'brought', 'vessel', 'vessel', 'vessel', 'entering', 'entering', ';--', ';--', 'blue', 'blue', 'thou', 'thou', 'thou', 'thou', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'my', 'however', 'however', 'however', 'however', 'order', 'order', 'order', 'order', 'order', 'only', 'only', 'only', 'only', 'only', 'mouth', 'mouth', 'mouth', 'mouth', 'mouth', 'how', 'how', 'how', 'how', 'how', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'chace', 'chace', 'seemed', 'seemed', 'seemed', 'entry', 'entry', 'entry', 'last', 'last', 'last', 'last', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'history', 'history', 'history', 'two', 'two', 'two', 'two', 'two', 'two', 'two', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'set', 'set', 'set', 'set', 'armed', 'armed', 'window', 'window', 'passage', 'passage', 'passage', 'purse', 'purse', 'here', 'here', 'here', 'here', 'here', 'here', 'here', 'here', 'here', 'american', 'american', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'image', 'image', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'be', 'said', 'said', 'said', 'said', 'said', 'once', 'once', 'once', 'once', 'once', 'ay', 'ay', 'ay', 'though', 'though', 'though', 'though', 'though', 'though', 'though', 'though', 'ribs', 'ribs', 'thing', 'thing', 'thing', 'thing', 'find', 'find', 'find', 'find', 'boats', 'boats', 'boats', 'ever', 'ever', 'ever', 'ever', 'ever', 'ever', 'ever', 'ever', 'same', 'same', 'same', 'same', 'same', 'extracts', 'extracts', 'extracts', 'else', 'else', 'else', '.--', '.--', '.--', '.--', 'particular', 'particular', 'particular', 'again', 'again', 'sail', 'sail', 'sail', 'sail', 'mast', 'mast', 'mast', 'mast', 'mast', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'told', 'told', 'told', 'told', 'besides', 'besides', 'besides', 'besides', 'wind', 'wind', 'wind', 'north', 'north', 'say', 'say', 'say', 'say', 'say', 'maketh', 'maketh', 'maketh', 'years', 'years', 'years', 'years', 'years', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'their', 'whenever', 'whenever', 'whenever', 'whenever', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'as', 'hear', 'hear', 'found', 'found', 'found', 'found', 'account', 'account', 'account', 'must', 'must', 'must', 'must', 'must', 'does', 'does', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'whale', 'euroclydon', 'euroclydon', 'euroclydon', 'euroclydon', 'tell', 'tell', 'tell', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'but', 'bright', 'bright', 'bright', 'country', 'country', 'country', 'men', 'men', 'men', 'men', 'almost', 'almost', 'almost', 'almost', 'almost', 'spouter', 'spouter', 'spouter', 'spouter', 'than', 'than', 'than', 'than', 'than', 'than', 'than', 'than', 'than', 'soul', 'soul', 'supplied', 'supplied', 'tears', 'tears', 'supper', 'supper', 'yourself', 'yourself', 'voyage', 'voyage', 'voyage', 'voyage', 'voyage', 'voyage', 'voyage', 'voyage', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'stranded', 'stranded', 'less', 'less', 'who', 'who', 'who', 'who', 'who', 'who', 'looked', 'looked', 'looked', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'money', 'money', 'money', 'money', 'green', 'green', 'after', 'after', 'grow', 'grow', 'grow', 'make', 'make', 'make', 'make', 'sperma', 'sperma', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'like', 'half', 'half', 'half', 'penny', 'penny', 'oil', 'oil', 'oil', 'over', 'over', 'over', 'over', 'over', 'over', 'over', 'over', 'deep', 'deep', 'deep', 'deep', 'hands', 'hands', 'hill', 'hill', 'shore', 'shore', 'shore', 'goes', 'goes', 'think', 'think', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'they', 'ye', 'ye', 'ye', 'ye', 'ye', 'down', 'down', 'down', 'down', 'down', 'down', 'down', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'between', 'between', 'heads', 'heads', 'artist', 'artist', 'thinks', 'thinks', 'swallow', 'swallow', 'life', 'life', 'life', 'fifty', 'fifty', 'commodore', 'commodore', 'fixed', 'fixed', 'did', 'did', 'did', 'did', 'did', 'did', 'did', 'form', 'form', 'huge', 'huge', '--\"', '--\"', '--\"', '--\"', 'would', 'would', 'would', 'would', 'would', 'would', 'would', 'would', 'would', 'would', 'own', 'own', 'own', 'own', 'own', 'hand', 'hand', 'hand', 'hand', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'water', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'glass', 'glass', 'glass', 'behind', 'behind', 'killed', 'killed', 'killed', 'killed', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'going', 'going', 'going', 'going', 'way', 'way', 'way', 'way', 'way', 'way', 'way', 'way', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'old', 'altogether', 'altogether', ',\"', ',\"', ',\"', ',\"', ',\"', ',\"', 'itself', 'itself', 'because', 'because', 'because', 'because', 'arched', 'arched', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'enter', 'enter', 'can', 'can', 'can', 'can', 'can', 'can', 'can', 'monster', 'monster', 'monster', 'being', 'being', 'being', 'being', 'being', 'being', 'being', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'this', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'whales', 'streets', 'streets', 'streets', 'streets', 'nigh', 'nigh', 'sleep', 'sleep', 'sleep', 'animal', 'animal', 'animal', 'animal', 'sailor', 'sailor', 'sailor', 'sailor', 'light', 'light', 'light', 'vast', 'vast', 'vast', 'vast', 'vast', 'right', 'right', 'right', 'right', 'right', 'right', 'such', 'such', 'such', 'such', 'such', 'such', 'such', 'such', 'such', 'lay', 'lay', 'themselves', 'themselves', 'land', 'land', 'land', 'land', 'land', 'land', 'ceti', 'ceti', 'immense', 'immense', 'man', 'man', 'man', 'man', 'man', 'man', 'towards', 'towards', 'towards', 'towards', 'rather', 'rather', 'rather', 'rather', 'quantity', 'quantity', 'then', 'then', 'then', 'then', 'then', 'craft', 'craft', 'craft', 'craft', 'ocean', 'ocean', 'ocean', 'ocean', 'ocean', 'ocean', 'ocean', 'should', 'should', 'should', 'should', 'should', 'should', 'also', 'also', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'she', 'she', 'she', 'she', 'dim', 'dim', 'dim', '...', '...', '...', '...', '...', '...', '...', 'high', 'high', 'high', 'high', 'well', 'well', 'well', 'well', 'well', 'known', 'known', 'known', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'was', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'upon', 'till', 'till', 'till', 'mighty', 'mighty', 'poor', 'poor', 'poor', 'poor', 'poor', 'globe', 'globe', 'globe', 'globe', 'city', 'city', 'city', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', 'hearts', 'hearts', '),', '),', '),', '),', 'him', 'him', 'him', 'him', 'him', 'him', 'him', 'him', 'him', 'shall', 'shall', 'shall', 'shall', 'young', 'young', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'for', 'winds', 'winds', 'within', 'within', 'within', 'within', 'within', 'sir', 'sir', 'sir', 'sir', 'sir', 'sir', 'stern', 'stern', 'sign', 'sign', 'sign', 'sign', 'bag', 'bag', 'things', 'things', 'things', 'things', 'heart', 'heart', 'body', 'body', 'body', 'beneath', 'beneath', 'your', 'your', 'your', 'your', 'your', 'your', 'your', 'your', 'your', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'sperm', 'sperm', 'sperm', 'sperm', 'sperm', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'leviathan', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'voyages', 'voyages', 'voyages', 'harpooneer', 'harpooneer', 'harpooneer', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'harpoons', 'harpoons', 'house', 'house', 'house', 'house', 'mind', 'mind', 'floating', 'floating', 'get', 'get', 'get', 'get', 'get', 't', 't', 't', 't', 't', 't', 'letter', 'letter', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'had', 'stone', 'stone', 'fish', 'fish', 'fish', 'fish', 'fish', 'door', 'door', 'door', 'door', 'thought', 'thought', 'thought', 'thought', 'thought', 'thought', 'either', 'either', 'either', 'either', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'to', 'few', 'few', 'true', 'true', 'true', 'true', 'gathered', 'gathered', 'around', 'around', 'around', 'inn', 'inn', 'inn', 'whaling', 'whaling', 'whaling', 'whaling', 'whaling', 'whaling', 'whaling', 'whaling', 'forty', 'forty', 'new', 'new', 'new', 'new', 'new', 'new', 'its', 'its', 'its', 'its', 'its', 'its', 'near', 'near', 'near', 'near', 'each', 'each', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'some', 'let', 'let', 'let', 'let', 'let', 'day', 'day', 'aloft', 'aloft', 'doubtless', 'doubtless', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', ';', 'ibid', 'ibid', 'black', 'black', 'black', 'black', 'about', 'about', 'about', 'about', 'about', 'about', 'about', 'about', 'about', 'moving', 'moving', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'he', 'an', 'an', 'an', 'an', 'an', 'an', 'an', 'an', 'an', 'an', '?\"', '?\"', '?\"', '?\"', 'gone', 'gone', 'gone', 'lead', 'lead', 'passenger', 'passenger', 'passenger', 'tempestuous', 'tempestuous', 'myself', 'myself', 'myself', 'myself', 'myself', 'sleeps', 'sleeps', 'making', 'making', 'red', 'red', 'others', 'others', 'ship', 'ship', 'ship', 'ship', 'ship', 'ship', 'ship', 'ship', 'without', 'without', 'without', 'without', 'without', 'been', 'been', 'been', 'been', 'been', 'been', 'been', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'might', 'might', 'might', 'might', 'might', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'these', 'these', 'these', 'these', 'these', 'these', 'these', 'these', 'these', 'these', 'lazarus', 'lazarus', 'lazarus', 'stream', 'stream', 'stream', ':', ':', ':', ':', 'matter', 'matter', 'matter', 'open', 'open', 'open', 'open', 'open', 'look', 'look', 'look', 'look', 'never', 'never', 'never', 'never', 'never', 'never', 'wild', 'wild', 'three', 'three', 'three', 'three', 'blows', 'blows', 'blows', 'too', 'too', 'too', 'too', 'too', 'too', 'through', 'through', 'through', 'through', 'through', 'through', 'boat', 'boat', 'boat', 'boat', 'large', 'large', 'large', 'large', 'room', 'room', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'by', 'miles', 'miles', 'miles', 'miles', 'teeth', 'teeth', 'teeth', 'long', 'long', 'long', 'long', 'long', 'long', 'ago', 'ago', 'ago', 'ago', 'feet', 'feet', 'feet', 'feet', 'feet', 'many', 'many', 'many', 'many', 'low', 'low', 'low', 'low', 'thomas', 'thomas', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'all', 'more', 'more', 'more', 'more', 'more', 'more', 'more', 'more', 'board', 'board', 'himself', 'himself', 'himself', 'himself', 'put', 'put', 'put', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'at', 'chapter', 'chapter', 'chapter', 'destroyed', 'destroyed', 'bones', 'bones', 'night', 'night', 'night', 'night', 'night', 'night', 'has', 'has', 'has', 'has', 'still', 'still', 'still', 'still', 'street', 'street', 'street', 'jolly', 'jolly', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'there', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'eyes', 'eyes', 'death', 'death', 'death', 'death', 'persons', 'persons', 'stop', 'stop', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'full', 'full', 'full', 'passengers', 'passengers', 'further', 'further', 'much', 'much', 'much', 'much', 'much', 'much', 'whose', 'whose', 'portentous', 'portentous', 'requires', 'requires', 'wide', 'wide', 'wide', 'wide', 'ishmael', 'ishmael', 'ishmael', 'ishmael', 'another', 'another', 'length', 'length', 'off', 'off', 'off', 'off', 'off', 'off', 'battle', 'battle', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'why', 'why', 'why', 'why', 'why', 'parts', 'parts', 'will', 'will', 'will', 'will', 'will', 'will', 'will', 'will', 'will', 'works', 'works', 'run', 'run', 'made', 'made', 'made', 'made', 'webster', 'webster', 'pains', 'pains', 'seas', 'seas', 'seas', 'seas', 'those', 'those', 'those', 'those', 'those', 'those', 'while', 'while', 'while', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'we', 'ice', 'ice', 'give', 'give', 'give', 'give', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'fishes', 'fishes', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', ')', ')', ')', ')', ')', ')', ')', 'her', 'her', 'her', 'her', 'jaws', 'jaws', 'jaws', 'jaws', 'bed', 'bed', 'pale', 'pale', 'compare', 'compare', 'd', 'd', 'd', 'd', 'd', 'place', 'place', 'place', 'place', 'place', 'place', \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", \"'\", 'days', 'days', 'out', 'out', 'out', 'out', 'out', 'out', 'out', 'out', 'out', 'out', 'out', 'whether', 'whether', 'whether', 'whether', 'before', 'before', 'before', 'before', 'before', 'before', 'before', 'created', 'created', 'saw', 'saw', 'saw', 'saw', 'saw', 'saw', 'saw', 'saw', 'sometimes', 'sometimes', 'ha', 'ha', 'beast', 'beast', 'our', 'our', 'our', 'our', 'four', 'four', 'time', 'time', 'time', 'time', 'time', 'time', 'time', 'even', 'even', 'even', 'even', 'nuee', 'nuee', 'nuee', 'cape', 'cape', 'cape', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', 'into', 'into', 'into', 'into', 'into', 'into', 'into', 'into', 'into', 'into', 'part', 'part', 'part', 'part', 'part', 'nothing', 'nothing', 'nothing', '!\"', '!\"', '!\"', '!\"', 'round', 'round', 'round', 'round', 'round', 'round', 'paying', 'paying', 'stood', 'stood', 'stood', 'something', 'something', 'something', 'something', 'something', 'head', 'head', 'head', 'head', 'head', 'head', 'head', 'head', 'coffin', 'coffin', 'coffin', 'stand', 'stand', 'stand', 'stand', 'stand', 'every', 'every', 'every', 'every', 'every', 'every', 'every', 'broiled', 'broiled', 'bedford', 'bedford', 'bedford', 'any', 'any', 'any', 'any', 'any', 'any', 'strong', 'strong', 'strong', 'come', 'come', 'come', 'come', 'come', 'could', 'could', 'could', 'could', 'could', 'called', 'called', 'called', 'called', 'jonah', 'jonah', 'sort', 'sort', 'sort', 'sort', 'sort', 'sort', 'cook', 'cook', 'cook', 'cannot', 'cannot', 'jaw', 'jaw', 'god', 'god', 'god', 'purpose', 'purpose', 'especially', 'especially', 'don', 'don', 'glasses', 'glasses', 'glasses', 'us', 'us', 'us', 'us', 'us', 'grand', 'grand', 'grand', 'grand', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'if', 'spermacetti', 'spermacetti', 'e', 'e', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'now', 'idea', 'idea', 'idea', 'most', 'most', 'most', 'most', 'most', 'most', 'most', 'most', 'am', 'am', 'am', 'am', 'very', 'very', 'very', 'very', 'very', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'island', 'island', 'northern', 'northern', 'away', 'away', 'thousand', 'thousand', 'dictionary', 'dictionary', 'go', 'go', 'go', 'go', 'go', 'go', 'go', 'go', 'go', 'go', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'name', 'name', 'narrative', 'narrative', 'narrative', 'late', 'late', 'late', 'monsters', 'monsters', 'take', 'take', 'take', 'take', 'take', 'take', 'take', 'take', 'sword', 'sword', 'sword', 'town', 'town', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'his', 'under', 'under', 'under', '?--', '?--', '?--', '?--', 'first', 'first', 'first', 'first', 'first', 'first', 'first', 'first', 'may', 'may', 'may', 'may', 'may', 'may', 'may', ',--', ',--', ',--', 'king', 'king', 'king', 'king', 'king', 'seen', 'seen', 'seen', 'enough', 'enough', 'enough', 'enough', 'ships', 'ships', 'ships', 'ships', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', 'sea', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', '.\"', 'monstrous', 'monstrous', 'monstrous', 'view', 'view', 'view', 'lord', 'lord', 'frost', 'frost', 'frost', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'great', 'sub', 'sub', 'sub', 'sub', 'sub', 'ten', 'ten', 'ten', 'point', 'point', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'south', 'south', 'south', 'far', 'far', 'far', 'far', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'nantucket', 'earth', 'earth', 'do', 'do', 'do', 'do', 'do', 'do', 'pacific', 'pacific', 'looking', 'looking', 'looking', 'side', 'side', 'side', 'side', 'sing', 'sing', 'tail', 'tail', 'tail', 'royal', 'royal']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVtb0mic7okz",
        "colab_type": "code",
        "outputId": "353c3b20-4074-4ea1-b5e6-fe95a86b641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(vocab), len(unigram_table))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "478 3500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC7ha5lm_UYc",
        "colab_type": "text"
      },
      "source": [
        "# **Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPmASeWB_W_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negative_sampling(targets, unigram_table, k):\n",
        "    batch_size = targets.size(0)\n",
        "    neg_samples = []\n",
        "    for i in range(batch_size):\n",
        "        nsample = []\n",
        "        target_index = targets[i].data.tolist()[0]\n",
        "        while len(nsample) < k:  # k = num of sampling\n",
        "            neg = random.choice(unigram_table)\n",
        "            if word2index[neg] == target_index:     \n",
        "                continue\n",
        "            nsample.append(neg)\n",
        "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "    \n",
        "    return torch.cat(neg_samples)     #concatenates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAYtc28A2KJ",
        "colab_type": "text"
      },
      "source": [
        "# **Modeling**\n",
        "\n",
        "\n",
        "![skip-gram model](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![대체 텍스트](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/02.skipgram-objective.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw_x7YXRA_nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, projection_dim):\n",
        "        super(SkipgramNegSampling, self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # center embedding\n",
        "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # out embedding\n",
        "        self.logsigmoid = nn.LogSigmoid()\n",
        "                \n",
        "        initrange = (2.0 / (vocab_size + projection_dim))**0.5 # Xavier init\n",
        "        self.embedding_v.weight.data.uniform_(-initrange, initrange) # init\n",
        "        self.embedding_u.weight.data.uniform_(-0.0, 0.0) # init\n",
        "        \n",
        "    def forward(self, center_words, target_words, negative_words):\n",
        "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
        "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
        "        \n",
        "        neg_embeds = -self.embedding_u(negative_words) # B x K x D   k = num of negative sampling\n",
        "        \n",
        "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
        "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1) # BxK -> Bx1\n",
        "        \n",
        "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
        "        \n",
        "        return -torch.mean(loss)\n",
        "    \n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "        \n",
        "        return embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg_xFERTB6BU",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIKrQT4JB8GN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 30 \n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 100\n",
        "NEG = 10 # Num of Negative Sampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmWjxt90B_Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = []\n",
        "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyyMZD2WCEDO",
        "colab_type": "code",
        "outputId": "67183f9b-5444-4105-f6ec-fec456b52790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "for epoch in range(EPOCH):\n",
        "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "        \n",
        "        inputs, targets = zip(*batch)\n",
        "        \n",
        "        inputs = torch.cat(inputs) # B x 1\n",
        "        targets = torch.cat(targets) # B x 1\n",
        "        negs = negative_sampling(targets, unigram_table, NEG)\n",
        "        model.zero_grad()\n",
        "\n",
        "        loss = model(inputs, targets, negs)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        losses.append(loss.data.tolist())\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
        "        losses = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0, mean_loss : 1.06\n",
            "Epoch : 10, mean_loss : 0.86\n",
            "Epoch : 20, mean_loss : 0.80\n",
            "Epoch : 30, mean_loss : 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al_-FzyrCKui",
        "colab_type": "text"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJeIrSb_CMxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_similarity(target, vocab):\n",
        "    target_V = model.prediction(prepare_word(target, word2index))\n",
        "    similarities = []\n",
        "    for i in range(len(vocab)):\n",
        "        if vocab[i] == target: \n",
        "            continue\n",
        "        \n",
        "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
        "        \n",
        "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
        "        similarities.append([vocab[i], cosine_sim])\n",
        "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOhDlBrjCO6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = random.choice(list(vocab))\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT30HLR6CQXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_similarity(test, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}